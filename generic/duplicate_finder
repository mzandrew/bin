#!/bin/bash -e

# original from http://www.commandlinefu.com/commands/view/3555/find-duplicate-files-based-on-size-first-then-md5-hash
#find -not -empty -type f -printf "%s\n" | sort -rn | uniq -d | xargs -I{} -n1 find -type f -size {}c -print0 | xargs -0 md5sum | sort | uniq -w32 --all-repeated=separate

# the following version is by mza (variation on the theme above)
# avoids considering duplicates in dirs: .git .svn proc dev
# avoids considering files below a certain size
# writes out a script to delete all duplicate files newer than the first one found (by modification date)
# updated 2016-06-16
# updated 2016-06-27 to fix bug where it would only catch duplicates if they matched the oldest file found with that size
# updated 2022-03-21 to work on just a given filespec/extention

# user options:
declare ext_list=""
#ext_list="tar zip rar dmg avi mkv mp4 m4v m4a iso vob divx m4b cbr pdf mpg"
declare -i lower_size_limit=10000000 # in bytes
#lower_size_limit=0 # in bytes
declare action="#rm" # if you'd prefer it to leave everything commented out by default
action="rm" # if you like to live dangerously
#action="svn remove" # if you're in a repo situation

declare prune_string="-type d ( -name proc -o -name dev -o -name \.svn -o -name \.git -o -name \@eaDir ) -prune"
declare file_sizes
declare output_script_name
declare -i actual_total_potential_savings=0
declare -i actual_total_numdups=0
declare ext=""

function go {
	if [ ! -z "$ext" ]; then
		echo "working only on: $ext"
		output_script_name="script_to_remove_all_but_oldest_duplicate.${ext}.sh"
		#file_sizes=$(find -type f -iname "*.$ext" -printf "%s\n" | sort -rn | uniq -d)
		file_sizes=$(find "${@}" $prune_string -o -type f -iname "*.$ext" -size +${lower_size_limit}c -printf "%s\n" | sort -rn | uniq -d)
	else
		output_script_name="script_to_remove_all_but_oldest_duplicate.sh"
		#file_sizes=$(find -type f                 -printf "%s\n" | sort -rn | uniq -d)
		file_sizes=$(find "${@}" $prune_string -o -type f                 -size +${lower_size_limit}c -printf "%s\n" | sort -rn | uniq -d)
	fi
	echo "#/bin/bash -e" >> "$output_script_name"
	chmod +x "$output_script_name"
	#echo $file_sizes
	local -i total_potential_savings=0
	local -i total_numdups=0
	for size in $file_sizes; do
		echo
		#echo "$size bytes"
		local script_string=""
		local list_of_files_to_potentially_remove=""
		local -i filecount=0 # filecount increments for each file that is $size bytes
		local -i numdups=0 # numdups increments for each duplicate that is found
		local -i maxhash=0 # maxhash increments each time a new hash is found
		local -i k=0 # hash counter
		local -a hash=()
		local is_dup="  "
		local old_IFS="$IFS"
		IFS=$'\n'
		#for filename in $(IFS="$old_IFS"; find "${@}" $prune_string -o -type f -size ${size}c -printf "%TY-%Tm-%Td+%TH:%TM %p\n" | sort -k 1n | colrm 1 17); do
		local listy
		if [ ! -z "$ext" ]; then
			listy=$(IFS="$old_IFS"; find "${@}" $prune_string -o -type f -iname "*.$ext" -size ${size}c -printf "%TY-%Tm-%Td+%TH:%TM %p\n" | sort -k 1n | colrm 1 17)
		else
			listy=$(IFS="$old_IFS"; find "${@}" $prune_string -o -type f                 -size ${size}c -printf "%TY-%Tm-%Td+%TH:%TM %p\n" | sort -k 1n | colrm 1 17)
		fi
		for filename in $listy; do
			local md5sum=$(md5sum "$filename" | colrm 33)
			is_dup="  "
			if [ $filecount -eq 0 ]; then
				hash[0]="$md5sum"
			else
				for k in $(seq 0 $maxhash); do
					if [ "$md5sum" == "${hash[$k]}" ]; then
						list_of_files_to_potentially_remove="$list_of_files_to_potentially_remove \"$filename\""
						numdups=$((numdups+1))
						is_dup="$(printf "%2d" $k)"
					fi
				done
				if [ "$is_dup" == "  " ]; then
					maxhash=$((maxhash+1))
					hash[$maxhash]="$md5sum"
				fi
			fi
			if [ "${filename:0:2}" != "./" ]; then
				if [ "${filename:0:1}" != "/" ]; then
					filename="./$filename"
				fi
			fi
			#echo "$filename"
			local result=$(IFS="$old_IFS"; find $prune_string -o -wholename "$filename" -printf "%TY-%Tm-%Td+%TH:%TM %12s $md5sum $is_dup %p\n")
			echo "$result"
			script_string="$script_string\n# $result"
			filecount=$((filecount+1))
		done
		IFS="$old_IFS"
		script_string="$script_string\n# uncomment the following line to delete all but the oldest of the duplicate files immediately above:"
		script_string="$script_string\n${action} -v$list_of_files_to_potentially_remove"
		if [ $numdups -gt 0 ]; then
			echo -e "${script_string}" >> "$output_script_name"
		fi
		total_potential_savings=$((total_potential_savings+numdups*size))
		total_numdups=$((total_numdups+numdups))
	done
	echo >> "$output_script_name"
	echo
	echo "potential savings:  $total_potential_savings bytes in $total_numdups files"
	echo "# potential savings:  $total_potential_savings bytes in $total_numdups files" >> "$output_script_name"
	actual_total_potential_savings=$((actual_total_potential_savings+total_potential_savings))
	actual_total_numdups=$((actual_total_numdups+total_numdups))
}

if [ ${#ext_list} -gt 0 ]; then
	for ext in $ext_list; do
		go
	done
else
	go
fi

echo
echo "total potential savings:  $actual_total_potential_savings bytes in $actual_total_numdups files"

#if [ ${action:0:1} == "#" ]; then
#	echo
#	echo "now edit file \"$output_script_name\" and only uncomment the lines you're sure you want"
#	echo "(suggest useful vim expression \":s/^#rm/rm/\" (etc) and then repeatedly \"/\" and \"&\")"
#	echo "then do \"bash $output_script_name\" to actually do it"
#fi

